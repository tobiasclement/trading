{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing DataLoaders for each model. These models include rule-based, vanilla DQN and encoder-decoder DQN.\n",
    "from DataLoader.DataLoader import YahooFinanceDataLoader\n",
    "from DataLoader.DataForPatternBasedAgent import DataForPatternBasedAgent\n",
    "from DataLoader.DataAutoPatternExtractionAgent import DataAutoPatternExtractionAgent\n",
    "from DataLoader.DataSequential import DataSequential \n",
    "\n",
    "from DeepRLAgent.MLPEncoder.Train import Train as SimpleMLP\n",
    "from DeepRLAgent.SimpleCNNEncoder.Train import Train as SimpleCNN\n",
    "from EncoderDecoderAgent.GRU.Train import Train as gru\n",
    "from EncoderDecoderAgent.CNN.Train import Train as cnn\n",
    "from EncoderDecoderAgent.CNN2D.Train import Train as cnn2d\n",
    "from EncoderDecoderAgent.CNNAttn.Train import Train as cnn_attn\n",
    "from EncoderDecoderAgent.CNN_GRU.Train import Train as cnn_gru\n",
    "\n",
    "\n",
    "# Imports for Deep RL Agent\n",
    "from DeepRLAgent.VanillaInput.Train import Train as DeepRL\n",
    "\n",
    "# Imports for RL Agent with n-step SARSA\n",
    "from RLAgent.Train import Train as RLTrain\n",
    "\n",
    "# Imports for Rule-Based\n",
    "from PatternDetectionInCandleStick.LabelPatterns import label_candles\n",
    "from PatternDetectionInCandleStick.Evaluation import Evaluation\n",
    "\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kaleido.scopes.plotly import PlotlyScope\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "CURRENT_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BATCH_SIZE` is set to 10 based on some analysis on the experiments. You can change it to whatever you want. `n_step` is the cumulative reward of n-steps in the future. `initial_investment` is the amount invested at the beginning of the process of trading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "GAMMA=0.7\n",
    "n_step = 10\n",
    "\n",
    "initial_investment = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_portfolios` and `test_portfolios` are dictionaries for saving the portfolio result of different models. The `key` of the dictionary is the model name and the `value` is a list of values showing the value of portfolio at each time-step. The `window_size_experiment` is the dictionary for storing the results of the experiments done for evaluating the effect of different window-sizes on portfolio values.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portfolios = {}\n",
    "test_portfolios = {}\n",
    "window_size_experiment = {}\n",
    "window_sizes = [3, 5, 8, 10, 12, 15, 20, 25, 30, 40, 50, 75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two helper functions are for storing the portfolio result of diffrent experiments given the `model_name` and a list of portfolio values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_train_portfo(model_name, portfo):\n",
    "    counter = 0\n",
    "    key = f'{model_name}'\n",
    "    while key in train_portfolios.keys():\n",
    "        counter += 1\n",
    "        key = f'{model_name}{counter}'\n",
    "        \n",
    "    train_portfolios[key] = portfo\n",
    "\n",
    "def add_test_portfo(model_name, portfo):\n",
    "    counter = 0\n",
    "    key = f'{model_name}'\n",
    "    while key in test_portfolios.keys():\n",
    "        counter += 1\n",
    "        key = f'{model_name}{counter}'\n",
    "    \n",
    "    test_portfolios[key] = portfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following blocks are for choosing a specific data to run the experiments. For example if you want to run the experiments on `BTC-USD` you should only run the first block, for `GOOGL` run the second block, blah blah blah. As also discussed in the documentation of the YahooFinanceDataLoader, you can set the `begin_date` and `end_date` to select a specific period from the original data. Also you can set the `split_point` which is a date of splitting train and test data. If your origianl file has changed or you are running the data for the first time, set the `load_from_file` to `false`. Otherwise set it to `True` to load from the preprocessed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BTC-USD\n",
    "\n",
    "DATASET_NAME = 'AAPL'\n",
    "DATASET_FOLDER = r'AAPL'\n",
    "FILE = r'AAPL.csv'\n",
    "#data_loader = YahooFinanceDataLoader(DATASET_FOLDER, FILE, '2020-01-02', load_from_file=True)\n",
    "\n",
    "data_loader = YahooFinanceDataLoader(1,\n",
    "                                   'AAPL',\n",
    "                                   split_point='2020-01-02',\n",
    "                                   begin_date='2017-01-03',\n",
    "                                   end_date='2023-05-12',\n",
    "                                   load_from_file=True,\n",
    "                                   )\n",
    "transaction_cost = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block is for choosing the observation-space mode. `state_mode = 1` is when the observation space contains only the Open, High, Low, and Close data. `state_mode = 2` also has a `trend` showing the trend of the stock before the candle stick at a specific time-step. `state_mode = 3` has OHLC and trend plus the size of the body, upper and lower shadow of the candle. `state_mode = 4` contains only the trend of the stock before that time-step plus the size of the body of the candle, upper and lower shadows. `state_mode = 5` is a window of `window-size` OHLCs plus the trend of the stock in that window. Pay attention that the window_size here is only for `DataAutoPatternExtractionAgent` not for the `DataSequential` which is the data used for time-series models like CNN and GRU. `DataForPatternBasedAgent` uses the extracted patterns as observation space. This kind of observation space is mainly used by RL agent with SARSA-$\\lambda$ and one kind of DQN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent with Auto pattern extraction\n",
    "# State Mode\n",
    "state_mode = 1  # OHLC\n",
    "# state_mode = 2  # OHLC + trend\n",
    "# state_mode = 3  # OHLC + trend + %body + %upper-shadow + %lower-shadow\n",
    "window_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain_autoPatternExtractionAgent = DataAutoPatternExtractionAgent(1,data_loader.data_train, state_mode, 'action_encoder_decoder', device, GAMMA, n_step, BATCH_SIZE, window_size, transaction_cost)\n",
    "dataTest_autoPatternExtractionAgent = DataAutoPatternExtractionAgent(1, data_loader.data_test, state_mode, 'action_encoder_decoder', device, GAMMA, n_step, BATCH_SIZE, window_size, transaction_cost)\n",
    "dataTrain_patternBased = DataForPatternBasedAgent(1, data_loader.data_train, data_loader.patterns, 'action_deepRL', device, GAMMA, n_step, BATCH_SIZE, transaction_cost)\n",
    "dataTest_patternBased = DataForPatternBasedAgent(1, data_loader.data_test, data_loader.patterns, 'action_deepRL', device, GAMMA, n_step, BATCH_SIZE, transaction_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed before, when `state_mode` is set to 4, the observation space has the trend, length of the body, upper and lower shadow of the candle. Therefore, we call this `state_mode` as candle representation because it contains a representation of the candle stick, not its original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mode = 4  # trend + %body + %upper-shadow + %lower-shadow\n",
    "\n",
    "dataTrain_autoPatternExtractionAgent_candle_rep = DataAutoPatternExtractionAgent(1,data_loader.data_train, state_mode, 'action_encoder_decoder', device, GAMMA, n_step, BATCH_SIZE, window_size, transaction_cost)\n",
    "dataTest_autoPatternExtractionAgent_candle_rep = DataAutoPatternExtractionAgent(1,data_loader.data_test, state_mode, 'action_encoder_decoder', device, GAMMA, n_step, BATCH_SIZE, window_size, transaction_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following blocks are for loading windowed input for non-timeseries models. As we discussed above, the `state_mode` is 5 in this case and you should provide the window-size in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mode = 5  # window with k candles inside + the trend of those candles\n",
    "window_size = 20\n",
    "dataTrain_autoPatternExtractionAgent_windowed = DataAutoPatternExtractionAgent(1,data_loader.data_train, state_mode, 'action_encoder_decoder', device, GAMMA, n_step, BATCH_SIZE, window_size, transaction_cost)\n",
    "dataTest_autoPatternExtractionAgent_windowed = DataAutoPatternExtractionAgent(1, data_loader.data_test, state_mode, 'action_encoder_decoder', device, GAMMA, n_step, BATCH_SIZE, window_size, transaction_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block is for loading the data for encoder-decoder models with time-series encoder. The `window_size` here is the number of candles inside one single time-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 15\n",
    "\n",
    "dataTrain_sequential = DataSequential(1, data_loader.data_train,\n",
    "                           'action_encoder_decoder', device, GAMMA,\n",
    "                           n_step, BATCH_SIZE, window_size, transaction_cost)\n",
    "dataTest_sequential = DataSequential(1, data_loader.data_test,\n",
    "                          'action_encoder_decoder', device, GAMMA,\n",
    "                          n_step, BATCH_SIZE, window_size, transaction_cost)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is for running the experiments for RL agent with n-step SARSA algorithm. The following block is for setting the HPs for this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Decoder models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section dedicates to the Deep Q-Learning agent with encoder. `TARGET_UPDATE` is how frequent the policy network is hard-copied to the target network in terms of number of episodes. `n_actions` is set to 3 because we have `buy`, `sell` and `none` actions. `n_episodes` is the number of episodes to run the algorithm. `EPS` is the $\\epsilon$ in the $\\epsilon$-greedy method. Decoder models include: MLP, 1-layerd 1d CNN, 1-layerd 2d CNN, two layered 1d CNN, GRU, CNN-GRU, CNN-Attn (CNN with an attention layer). For details about these models, please refer to the paper and README file.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "EPS = 0.1\n",
    "# EPS_START = 0.9\n",
    "# EPS_END = 0.05\n",
    "# EPS_DECAY = 200\n",
    "\n",
    "ReplayMemorySize = 20\n",
    "\n",
    "TARGET_UPDATE = 5\n",
    "n_actions = 3\n",
    "# window_size = 20\n",
    "\n",
    "num_episodes = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following bocks, each block is for running the DQN algorithm in a specific condition. For each block, if you want to train from scratch, use the following two lines:\n",
    "```\n",
    "<name of the agent>.train(n_episodes)\n",
    "file_name = None\n",
    "```\n",
    "pay attention that the `file_name` should be set to `None` so that for the evaluation process, it does not load from file and instead use the recently trained agent. If you want to load from a specific model, set the `file_name` to the name of the file inside the `./Objects/<agent name>` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OHLC input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses the OHLC representation of candles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'AAPL; MLP; StateMode(1); WindowSize(20); TRAIN_TEST_SPLIT(True); BATCH_SIZE10; GAMMA0.7; EPSILON0.1; REPLAY_MEMORY_SIZE20; C5; N_SARSA10; EXPERIMENT.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m#simpleMLP.train(num_episodes)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m#file_name = None\u001b[39;00m\n\u001b[1;32m      7\u001b[0m file_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mAAPL; MLP; StateMode(1); WindowSize(20); TRAIN_TEST_SPLIT(True); BATCH_SIZE10; GAMMA0.7; EPSILON0.1; REPLAY_MEMORY_SIZE20; C5; N_SARSA10; EXPERIMENT.pkl\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(file_name)\n\u001b[1;32m     11\u001b[0m ev_simpleMLP \u001b[39m=\u001b[39m simpleMLP\u001b[39m.\u001b[39mtest(file_name\u001b[39m=\u001b[39mfile_name, action_name\u001b[39m=\u001b[39mdataTrain_autoPatternExtractionAgent\u001b[39m.\u001b[39maction_name,\n\u001b[1;32m     12\u001b[0m                                   initial_investment\u001b[39m=\u001b[39minitial_investment, test_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m simpleMLP_portfolio_train \u001b[39m=\u001b[39m ev_simpleMLP\u001b[39m.\u001b[39mget_daily_portfolio_value()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AAPL; MLP; StateMode(1); WindowSize(20); TRAIN_TEST_SPLIT(True); BATCH_SIZE10; GAMMA0.7; EPSILON0.1; REPLAY_MEMORY_SIZE20; C5; N_SARSA10; EXPERIMENT.pkl'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_classes = 64\n",
    "\n",
    "simpleMLP = SimpleMLP(1, data_loader, dataTrain_autoPatternExtractionAgent, dataTest_autoPatternExtractionAgent, DATASET_NAME, state_mode, window_size, transaction_cost, n_classes, BATCH_SIZE=BATCH_SIZE, GAMMA=GAMMA, ReplayMemorySize=ReplayMemorySize, TARGET_UPDATE=TARGET_UPDATE, n_step=n_step)\n",
    "\n",
    "#simpleMLP.train(num_episodes)\n",
    "#file_name = None\n",
    "file_name = 'AAPL; MLP; StateMode(1); WindowSize(20); TRAIN_TEST_SPLIT(True); BATCH_SIZE10; GAMMA0.7; EPSILON0.1; REPLAY_MEMORY_SIZE20; C5; N_SARSA10; EXPERIMENT.pkl'\n",
    "\n",
    "#here we call the test function, whicch is missing the functionality of loading a .pkl file (see BaseTrain.py)\n",
    "ev_simpleMLP = simpleMLP.test(file_name=file_name,\n",
    "                                  initial_investment=initial_investment, test_type='train')\n",
    "simpleMLP_portfolio_train = ev_simpleMLP.get_daily_portfolio_value()\n",
    "ev_simpleMLP = simpleMLP.test(file_name=file_name,\n",
    "                                  initial_investment=initial_investment, test_type='test')\n",
    "simpleMLP_portfolio_test = ev_simpleMLP.get_daily_portfolio_value()\n",
    "\n",
    "model_kind = 'MLP-vanilla'\n",
    "\n",
    "add_train_portfo(model_kind, simpleMLP_portfolio_train)\n",
    "add_test_portfo(model_kind, simpleMLP_portfolio_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Buy and Hold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buy the stock in the beginning of the process and hold it until the end without selling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataTrain_patternBased.data[dataTrain_patternBased.action_name] = 'buy'\n",
    "ev_BandH = Evaluation(dataTrain_patternBased.data, dataTrain_patternBased.action_name, initial_investment)\n",
    "print('train')\n",
    "ev_BandH.evaluate()\n",
    "BandH_portfolio_train = ev_BandH.get_daily_portfolio_value()\n",
    "\n",
    "dataTest_patternBased.data[dataTest_patternBased.action_name] = 'buy'\n",
    "ev_BandH = Evaluation(dataTest_patternBased.data, dataTest_patternBased.action_name, initial_investment)\n",
    "print('test')\n",
    "ev_BandH.evaluate()\n",
    "BandH_portfolio_test = ev_BandH.get_daily_portfolio_value()\n",
    "\n",
    "add_train_portfo('B&H', BandH_portfolio_train)\n",
    "add_test_portfo('B&H', BandH_portfolio_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the blocks related to `Action List`, you can represent the strategies generated by each model on a diagram. In order to do so, you should first run the block related to that model in the above, then run one of the blocks for related to `Action List` for plotting the strategy devised by that specific model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action List on candlestick chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, in order to plot a better representation, we limit the number of canldlestick in each plot to 100. You can increase or decrease this amount by changing the values of `begin` and `end` in the following block. The `begin` and `end` are indices to the original data to select the interval of data you want to see it's trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = 0\n",
    "end =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_test = dataTest_autoPatternExtractionAgent\n",
    "# data_test = dataTest_autoPatternExtractionAgent_windowed\n",
    "data_test = dataTest_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_test.data[data_test.data[data_test.action_name] == 'None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_num = 1\n",
    "RESULTS_PATH = f'TestResults/ActionList/{model_kind}/'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "    os.mkdir(RESULTS_PATH)\n",
    "\n",
    "while os.path.exists(f'{RESULTS_PATH}{DATASET_NAME};{model_kind};actions({experiment_num}).svg'):\n",
    "    experiment_num += 1\n",
    "\n",
    "fig_file = f'{RESULTS_PATH}{DATASET_NAME};{model_kind};actions({experiment_num}).svg'\n",
    "\n",
    "scope = PlotlyScope()\n",
    "\n",
    "df1 = data_loader.data_test_with_date[begin:end]\n",
    "actionlist = list(data_test.data[data_test.action_name][begin:end])\n",
    "df1[data_test.action_name] = actionlist\n",
    "\n",
    "buy = df1.copy()\n",
    "sell = df1.copy()\n",
    "none = df1.copy()\n",
    "\n",
    "# buy['action'] = [(c + o)/2 if a == 0 else None for a, o, c in zip(df2.action, df1.open, df1.close)]\n",
    "# sell['action'] = [(c + o)/2 if a == 2 else None for a, o, c in zip(df2.action, df1.open, df1.close)]\n",
    "# none['action'] = [(c + o)/2 if a == 1 else None for a, o, c in zip(df2.action, df1.open, df1.close)]\n",
    "\n",
    "buy['action'] = [(c + o)/2 if a == 'buy' else None for a, o, c in zip(df1[data_test.action_name], df1.open, df1.close)]\n",
    "sell['action'] = [(c + o)/2 if a == 'sell' else None for a, o, c in zip(df1[data_test.action_name], df1.open, df1.close)]\n",
    "none['action'] = [(c + o)/2 if a == 'None' else None for a, o, c in zip(df1[data_test.action_name], df1.open, df1.close)]\n",
    "\n",
    "\n",
    "data=[go.Candlestick(x=df1.index,\n",
    "                open=df1['open'],\n",
    "                high=df1['high'],\n",
    "                low=df1['low'],\n",
    "                close=df1['close'], increasing_line_color= 'lightgreen', decreasing_line_color= '#ff6961'),\n",
    "     go.Scatter(x=df1.index, y=buy.action, mode = 'markers', \n",
    "                marker=dict(color='green', colorscale='Viridis'), name=\"buy\"), \n",
    "     go.Scatter(x=df1.index, y=none.action, mode = 'markers', \n",
    "                marker=dict(color='blue', colorscale='Viridis'), name=\"none\"), \n",
    "     go.Scatter(x=df1.index, y=sell.action, mode = 'markers', \n",
    "                marker=dict(color='red', colorscale='Viridis'), name=\"sell\")]\n",
    "\n",
    "layout = go.Layout(\n",
    "    autosize=False,\n",
    "    width=900,\n",
    "    height=600)\n",
    "\n",
    "\n",
    "figSignal = go.Figure(data=data, layout=layout)\n",
    "figSignal.show()\n",
    "# with open(fig_file, \"wb\") as f:\n",
    "#     f.write(scope.transform(figSignal, format=\"svg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data (Plotted using seaborn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block plots the result portfolio of different models stored in the `train_portfolios` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_num = 1\n",
    "RESULTS_PATH = 'TestResults/Train/'\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "    os.mkdir(RESULTS_PATH)\n",
    "\n",
    "while os.path.exists(f'{RESULTS_PATH}{DATASET_NAME};train;EXPERIMENT({experiment_num}).jpg'):\n",
    "    experiment_num += 1\n",
    "\n",
    "fig_file = f'{RESULTS_PATH}{DATASET_NAME};train;EXPERIMENT({experiment_num}).jpg'\n",
    "\n",
    "sns.set(rc={'figure.figsize': (15, 7)})\n",
    "\n",
    "items = list(test_portfolios.keys())\n",
    "random.shuffle(items)\n",
    "\n",
    "first = True\n",
    "for k in items:\n",
    "    profit_percentage = [(train_portfolios[k][i] - train_portfolios[k][0])/train_portfolios[k][0] * 100 \n",
    "              for i in range(len(train_portfolios[k]))]\n",
    "    difference = len(train_portfolios[k]) - len(data_loader.data_train_with_date)\n",
    "    df = pd.DataFrame({'date': data_loader.data_train_with_date.index, \n",
    "                       'portfolio':profit_percentage[difference:]})\n",
    "    if not first:\n",
    "        df.plot(ax=ax, x='date', y='portfolio', label=k)\n",
    "    else:\n",
    "        ax = df.plot(x='date', y='portfolio', label=k)\n",
    "        first = False\n",
    "\n",
    "ax.set(xlabel='Time', ylabel='%Rate of Return')\n",
    "ax.set_title(f'%Rate of Return at each point of time for training data of {DATASET_NAME}')\n",
    "        \n",
    "plt.legend()\n",
    "# plt.savefig(fig_file, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data (Plotted using Seaborn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle = False\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block plots the results of test portfolios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "experiment_num = 1\n",
    "RESULTS_PATH = 'TestResults/Test/'\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "    os.mkdir(RESULTS_PATH)\n",
    "\n",
    "while os.path.exists(f'{RESULTS_PATH}{DATASET_NAME};test;EXPERIMENT({experiment_num}).jpg'):\n",
    "    experiment_num += 1\n",
    "\n",
    "fig_file = f'{RESULTS_PATH}{DATASET_NAME};test;EXPERIMENT({experiment_num}).jpg'\n",
    "\n",
    "sns.set(rc={'figure.figsize': (15, 7)})\n",
    "sns.set_palette(sns.color_palette(\"Paired\", 15))\n",
    "\n",
    "items = list(test_portfolios.keys())\n",
    "\n",
    "if shuffle:\n",
    "    random.shuffle(items)\n",
    "\n",
    "first = True\n",
    "for k in items:\n",
    "    profit_percentage = [(test_portfolios[k][i] - test_portfolios[k][0])/test_portfolios[k][0] * 100 \n",
    "                  for i in range(len(test_portfolios[k]))]\n",
    "    difference = len(test_portfolios[k]) - len(data_loader.data_test_with_date)\n",
    "    df = pd.DataFrame({'date': data_loader.data_test_with_date.index, \n",
    "                       'portfolio':profit_percentage[difference:]})\n",
    "    if not first:\n",
    "        df.plot(ax=ax, x='date', y='portfolio', label=k)\n",
    "    else:\n",
    "        ax = df.plot(x='date', y='portfolio', label=k)\n",
    "        first = False\n",
    "        \n",
    "ax.set(xlabel='Time', ylabel='%Rate of Return')\n",
    "ax.set_title(f'Comparing the %Rate of Return for different models '\n",
    "             f'at each point of time for test data of {DATASET_NAME}')\n",
    "plt.legend()\n",
    "plt.savefig(fig_file, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test window size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block plots a line showing the relation between the window-size and the profit in each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "\n",
    "# 300 represents number of points to make between T.min and T.max\n",
    "# xnew = np.linspace(3, 75, 300)\n",
    "\n",
    "experiment_num = 1\n",
    "RESULTS_PATH = 'TestResults/WindowSize/'\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "    os.mkdir(RESULTS_PATH)\n",
    "\n",
    "while os.path.exists(f'{RESULTS_PATH}{DATASET_NAME};WindowSize;test({experiment_num}).jpg'):\n",
    "    experiment_num += 1\n",
    "\n",
    "fig_file = f'{RESULTS_PATH}{DATASET_NAME};WindowSize;test({experiment_num}).jpg'\n",
    "\n",
    "sns.set(rc={'figure.figsize': (9, 5)})\n",
    "sns.set_palette(sns.color_palette(\"Paired\", 15))\n",
    "\n",
    "first = True\n",
    "for key, val in window_size_experiment.items():\n",
    "    total_returns = list(val.values())\n",
    "    \n",
    "    # Normalize returns:\n",
    "    total_returns = [(r - min(total_returns))/(max(total_returns) - min(total_returns)) for r in total_returns]\n",
    "#     f1 = interp1d(window_sizes, total_returns, kind='previous')\n",
    "    if first:\n",
    "#         ax = sns.lineplot(xnew, f1(xnew), label=key)\n",
    "        ax = sns.lineplot(window_sizes, total_returns, label=key)\n",
    "        first = False\n",
    "    else:\n",
    "        sns.lineplot(ax=ax, x=window_sizes, y=total_returns, label=key)\n",
    "#         sns.lineplot(ax=ax, x=xnew, y=f1(xnew), label=key)\n",
    "        \n",
    "ax.set(xlabel='WindowSize', ylabel='%Total Return')\n",
    "ax.set_title(f'The impact of window size on different models')\n",
    "plt.legend()\n",
    "# plt.savefig(fig_file, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block plots the relation between the window-size and the profit in each model in a heatmap diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_num = 1\n",
    "RESULTS_PATH = 'TestResults/WindowSize/'\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "    os.mkdir(RESULTS_PATH)\n",
    "\n",
    "while os.path.exists(f'{RESULTS_PATH}{DATASET_NAME};WindowSize;heatmap;test({experiment_num}).jpg'):\n",
    "    experiment_num += 1\n",
    "\n",
    "fig_file = f'{RESULTS_PATH}{DATASET_NAME};WindowSize;heatmap;test({experiment_num}).jpg'\n",
    "\n",
    "sns.set(rc={'figure.figsize': (10, 5)})\n",
    "\n",
    "normalized = {}\n",
    "for k1 in window_size_experiment.keys():\n",
    "    normalized[k1] = {}\n",
    "    total_returns = list(window_size_experiment[k1].values())\n",
    "    max_return = max(total_returns)\n",
    "    min_return = min(total_returns)\n",
    "    for k2 in window_size_experiment[k1].keys():\n",
    "        return_val = window_size_experiment[k1][k2]\n",
    "        normalized[k1][k2] = (return_val - min_return)/(max_return - min_return)\n",
    "        \n",
    "\n",
    "df = pd.DataFrame.transpose(pd.DataFrame.from_dict(normalized))\n",
    "sns.heatmap(df)\n",
    "\n",
    "plt.savefig(fig_file, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
